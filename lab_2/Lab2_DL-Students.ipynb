{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 1.5 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Group name:</b> # Elise  Dong, Ariane Horbach,\n",
    "Deeplearn44\n",
    " \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Generate your final report (export as HTML) and upload it on the submission website http://bigfoot-m1.eurecom.fr/teachingsub/login (using your deeplearnXX/password). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed and submitted by May 30th 2018 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%. Can you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks,  **LeNet-5**, to go to more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell below to load the MNIST data that comes with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting .../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting .../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting .../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('.../'+\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "epsilon = 1e-10 # this is a parameter you will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fb2248730f0>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fb1a1ef09e8>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fb1a1ef05c0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.3803922 , 0.37647063, 0.3019608 ,\n",
       "       0.46274513, 0.2392157 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.3529412 , 0.5411765 , 0.9215687 ,\n",
       "       0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 ,\n",
       "       0.9843138 , 0.9843138 , 0.9725491 , 0.9960785 , 0.9607844 ,\n",
       "       0.9215687 , 0.74509805, 0.08235294, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54901963,\n",
       "       0.9843138 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.7411765 , 0.09019608, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.8862746 , 0.9960785 , 0.81568635,\n",
       "       0.7803922 , 0.7803922 , 0.7803922 , 0.7803922 , 0.54509807,\n",
       "       0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 ,\n",
       "       0.5019608 , 0.8705883 , 0.9960785 , 0.9960785 , 0.7411765 ,\n",
       "       0.08235294, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.14901961, 0.32156864, 0.0509804 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.13333334,\n",
       "       0.8352942 , 0.9960785 , 0.9960785 , 0.45098042, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.32941177, 0.9960785 ,\n",
       "       0.9960785 , 0.9176471 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.32941177, 0.9960785 , 0.9960785 , 0.9176471 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.4156863 , 0.6156863 ,\n",
       "       0.9960785 , 0.9960785 , 0.95294124, 0.20000002, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09803922, 0.45882356, 0.8941177 , 0.8941177 ,\n",
       "       0.8941177 , 0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.9960785 , 0.94117653, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.26666668, 0.4666667 , 0.86274517,\n",
       "       0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.5568628 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.14509805, 0.73333335,\n",
       "       0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 , 0.8745099 ,\n",
       "       0.8078432 , 0.8078432 , 0.29411766, 0.26666668, 0.8431373 ,\n",
       "       0.9960785 , 0.9960785 , 0.45882356, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.4431373 , 0.8588236 , 0.9960785 , 0.9490197 , 0.89019614,\n",
       "       0.45098042, 0.34901962, 0.12156864, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.7843138 , 0.9960785 , 0.9450981 ,\n",
       "       0.16078432, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.6627451 , 0.9960785 ,\n",
       "       0.6901961 , 0.24313727, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.18823531,\n",
       "       0.9058824 , 0.9960785 , 0.9176471 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.48627454, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 0.9960785 , 0.9960785 ,\n",
       "       0.6509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.54509807, 0.9960785 , 0.9333334 , 0.22352943, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.8235295 , 0.9803922 , 0.9960785 ,\n",
       "       0.65882355, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.9490197 , 0.9960785 , 0.93725497, 0.22352943, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.34901962, 0.9843138 , 0.9450981 ,\n",
       "       0.3372549 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.01960784,\n",
       "       0.8078432 , 0.96470594, 0.6156863 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01568628, 0.45882356, 0.27058825,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example\n",
    "**y=softmax(Wx+b)** seen in the first lab. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the TensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 0.589759183\n",
      "Epoch:  02   =====> Loss= 0.382416810\n",
      "Epoch:  03   =====> Loss= 0.348814638\n",
      "Epoch:  04   =====> Loss= 0.331653515\n",
      "Epoch:  05   =====> Loss= 0.320566527\n",
      "Epoch:  06   =====> Loss= 0.312745708\n",
      "Epoch:  07   =====> Loss= 0.306496050\n",
      "Epoch:  08   =====> Loss= 0.301348280\n",
      "Epoch:  09   =====> Loss= 0.297495954\n",
      "Epoch:  10   =====> Loss= 0.294215277\n",
      "Epoch:  11   =====> Loss= 0.291212338\n",
      "Epoch:  12   =====> Loss= 0.288705647\n",
      "Epoch:  13   =====> Loss= 0.286202669\n",
      "Epoch:  14   =====> Loss= 0.284131975\n",
      "Epoch:  15   =====> Loss= 0.282426295\n",
      "Epoch:  16   =====> Loss= 0.280914684\n",
      "Epoch:  17   =====> Loss= 0.279375355\n",
      "Epoch:  18   =====> Loss= 0.277669499\n",
      "Epoch:  19   =====> Loss= 0.276561501\n",
      "Epoch:  20   =====> Loss= 0.275235734\n",
      "Epoch:  21   =====> Loss= 0.274281487\n",
      "Epoch:  22   =====> Loss= 0.273452084\n",
      "Epoch:  23   =====> Loss= 0.272506601\n",
      "Epoch:  24   =====> Loss= 0.271589055\n",
      "Epoch:  25   =====> Loss= 0.270429518\n",
      "Epoch:  26   =====> Loss= 0.269877336\n",
      "Epoch:  27   =====> Loss= 0.268595971\n",
      "Epoch:  28   =====> Loss= 0.268174959\n",
      "Epoch:  29   =====> Loss= 0.267311471\n",
      "Epoch:  30   =====> Loss= 0.266681935\n",
      "Epoch:  31   =====> Loss= 0.266048628\n",
      "Epoch:  32   =====> Loss= 0.265638510\n",
      "Epoch:  33   =====> Loss= 0.264647537\n",
      "Epoch:  34   =====> Loss= 0.264237271\n",
      "Epoch:  35   =====> Loss= 0.263399774\n",
      "Epoch:  36   =====> Loss= 0.263201364\n",
      "Epoch:  37   =====> Loss= 0.262724136\n",
      "Epoch:  38   =====> Loss= 0.262352673\n",
      "Epoch:  39   =====> Loss= 0.261849309\n",
      "Epoch:  40   =====> Loss= 0.261208101\n",
      "Epoch:  41   =====> Loss= 0.260660871\n",
      "Epoch:  42   =====> Loss= 0.260315765\n",
      "Epoch:  43   =====> Loss= 0.259529660\n",
      "Epoch:  44   =====> Loss= 0.259230063\n",
      "Epoch:  45   =====> Loss= 0.259045885\n",
      "Epoch:  46   =====> Loss= 0.258736033\n",
      "Epoch:  47   =====> Loss= 0.258363372\n",
      "Epoch:  48   =====> Loss= 0.257976095\n",
      "Epoch:  49   =====> Loss= 0.257842505\n",
      "Epoch:  50   =====> Loss= 0.257322951\n",
      "Epoch:  51   =====> Loss= 0.256926972\n",
      "Epoch:  52   =====> Loss= 0.256427098\n",
      "Epoch:  53   =====> Loss= 0.256064038\n",
      "Epoch:  54   =====> Loss= 0.256145156\n",
      "Epoch:  55   =====> Loss= 0.255590593\n",
      "Epoch:  56   =====> Loss= 0.255471106\n",
      "Epoch:  57   =====> Loss= 0.255297467\n",
      "Epoch:  58   =====> Loss= 0.254745091\n",
      "Epoch:  59   =====> Loss= 0.254530471\n",
      "Epoch:  60   =====> Loss= 0.254217789\n",
      "Epoch:  61   =====> Loss= 0.254188908\n",
      "Epoch:  62   =====> Loss= 0.253746596\n",
      "Epoch:  63   =====> Loss= 0.253621349\n",
      "Epoch:  64   =====> Loss= 0.253261554\n",
      "Epoch:  65   =====> Loss= 0.253192241\n",
      "Epoch:  66   =====> Loss= 0.252788377\n",
      "Epoch:  67   =====> Loss= 0.252579172\n",
      "Epoch:  68   =====> Loss= 0.252100699\n",
      "Epoch:  69   =====> Loss= 0.252011584\n",
      "Epoch:  70   =====> Loss= 0.251804518\n",
      "Epoch:  71   =====> Loss= 0.251558106\n",
      "Epoch:  72   =====> Loss= 0.251253170\n",
      "Epoch:  73   =====> Loss= 0.251291963\n",
      "Epoch:  74   =====> Loss= 0.250689053\n",
      "Epoch:  75   =====> Loss= 0.250810788\n",
      "Epoch:  76   =====> Loss= 0.250490728\n",
      "Epoch:  77   =====> Loss= 0.250464523\n",
      "Epoch:  78   =====> Loss= 0.250171463\n",
      "Epoch:  79   =====> Loss= 0.250029679\n",
      "Epoch:  80   =====> Loss= 0.249605328\n",
      "Epoch:  81   =====> Loss= 0.249238959\n",
      "Epoch:  82   =====> Loss= 0.249424003\n",
      "Epoch:  83   =====> Loss= 0.249293311\n",
      "Epoch:  84   =====> Loss= 0.249048523\n",
      "Epoch:  85   =====> Loss= 0.248746474\n",
      "Epoch:  86   =====> Loss= 0.248461067\n",
      "Epoch:  87   =====> Loss= 0.248310883\n",
      "Epoch:  88   =====> Loss= 0.248241955\n",
      "Epoch:  89   =====> Loss= 0.248294064\n",
      "Epoch:  90   =====> Loss= 0.247898929\n",
      "Epoch:  91   =====> Loss= 0.247881644\n",
      "Epoch:  92   =====> Loss= 0.247822072\n",
      "Epoch:  93   =====> Loss= 0.247470710\n",
      "Epoch:  94   =====> Loss= 0.247218989\n",
      "Epoch:  95   =====> Loss= 0.247057501\n",
      "Epoch:  96   =====> Loss= 0.246928980\n",
      "Epoch:  97   =====> Loss= 0.247037608\n",
      "Epoch:  98   =====> Loss= 0.246492701\n",
      "Epoch:  99   =====> Loss= 0.246699017\n",
      "Epoch:  100   =====> Loss= 0.246620943\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9251\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=lab_2/log_files/\"**\n",
    "- Click on \"Tensorboard web interface\" in Zoe  \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now more familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet-5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions  for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function weight_variable is used to initialize the weights for our Convolutional Network. It takes on the argument shape, which defines the dimension (1-D0 for the output tensor. The tf.truncate_normal function takes on the argument shape as well as a standard deviation. The standard deviation is a 0-D Tensor or Python value (dtype), which can be tweaked to achieve better results.\n",
    "\n",
    "The bias_variable function is used to initialize the bias for our Convolutional Network. The function takes on shape, which takes the dimensions of resulting tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/\n",
    "# https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "\n",
    "def LeNet5_Model(data,activation_function=tf.nn.relu):\n",
    "\n",
    "    # layer 1 param\n",
    "    conv1_weights = weight_variable([5,5,1,6])\n",
    "    conv1_bias = bias_variable([6])\n",
    "    \n",
    "    # layer 2 param\n",
    "    conv2_weights = weight_variable([5,5,6,16])\n",
    "    conv2_bias = bias_variable([16])\n",
    "    \n",
    "    # layer 3 param\n",
    "    layer3_weights = weight_variable([400, 120])\n",
    "    layer3_bias = bias_variable([120])\n",
    "    \n",
    "    # layer 4 param\n",
    "    layer4_weights = weight_variable([120, 84])\n",
    "    layer4_bias = bias_variable([84])\n",
    "    \n",
    "    # layer 5 param\n",
    "    layer5_weights = weight_variable([84, 10])\n",
    "    layer5_bias = bias_variable([10])\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('Model'):\n",
    "        with tf.name_scope('Layer1'):\n",
    "            conv1 = tf.nn.conv2d(input=data,filter=conv1_weights,strides=[1,1,1,1],padding='SAME')\n",
    "            print(conv1.shape)\n",
    "            relu1 = activation_function(conv1 + conv1_bias)\n",
    "            pool1 = tf.nn.max_pool(relu1,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='VALID')\n",
    "            print(pool1.shape)\n",
    "            \n",
    "        with tf.name_scope('Layer2'):\n",
    "            conv2 = tf.nn.conv2d(input=pool1,filter=conv2_weights,strides=[1,1,1,1],padding='VALID')\n",
    "            print(conv2.shape)\n",
    "            relu2 = activation_function(conv2 + conv2_bias)\n",
    "            pool2 = tf.nn.max_pool(relu2,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='VALID')   \n",
    "            print(pool2.shape)\n",
    "        \n",
    "        with tf.name_scope('Flatten'):\n",
    "            flat_inputs = tf.contrib.layers.flatten(pool2)\n",
    "            print(flat_inputs.shape)\n",
    "           \n",
    "        with tf.name_scope('Layer3'):\n",
    "            out3 = activation_function(tf.matmul(flat_inputs, layer3_weights) + layer3_bias)\n",
    "            \n",
    "        with tf.name_scope('Layer4'):\n",
    "            out4 = activation_function(tf.matmul(out3, layer4_weights) + layer4_bias)\n",
    "            \n",
    "        with tf.name_scope('Layer5'):\n",
    "            pred = tf.nn.softmax(tf.matmul(out4, layer5_weights) + layer5_bias) # Softmax\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in LeNet5: 61706\n"
     ]
    }
   ],
   "source": [
    "NParameters_LeNet5 = \\\n",
    "    5*5*1*6 + 6+ \\\n",
    "    5*5*6*16 + 16+ \\\n",
    "    400*120 + \\\n",
    "    120 + \\\n",
    "    120*84 + \\\n",
    "    84 + \\\n",
    "    84*10 + \\\n",
    "    10\n",
    "    \n",
    "    \n",
    "print('Number of parameters in LeNet5: %d'%NParameters_LeNet5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Function: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 28, 28, 6)\n",
      "(128, 14, 14, 6)\n",
      "(128, 10, 10, 16)\n",
      "(128, 5, 5, 16)\n",
      "(128, 400)\n",
      "Epoch:  01   =====> Loss= 2.286717658\n",
      "Epoch:  02   =====> Loss= 2.229259240\n",
      "Epoch:  03   =====> Loss= 2.137424312\n",
      "Epoch:  04   =====> Loss= 1.923655283\n",
      "Epoch:  05   =====> Loss= 1.439618085\n",
      "Epoch:  06   =====> Loss= 0.926344206\n",
      "Epoch:  07   =====> Loss= 0.675434008\n",
      "Epoch:  08   =====> Loss= 0.547201579\n",
      "Epoch:  09   =====> Loss= 0.471650872\n",
      "Epoch:  10   =====> Loss= 0.427180266\n",
      "Epoch:  11   =====> Loss= 0.392251058\n",
      "Epoch:  12   =====> Loss= 0.367934332\n",
      "Epoch:  13   =====> Loss= 0.346097705\n",
      "Epoch:  14   =====> Loss= 0.328136293\n",
      "Epoch:  15   =====> Loss= 0.313854165\n",
      "Epoch:  16   =====> Loss= 0.300735826\n",
      "Epoch:  17   =====> Loss= 0.285276640\n",
      "Epoch:  18   =====> Loss= 0.275655913\n",
      "Epoch:  19   =====> Loss= 0.266494973\n",
      "Epoch:  20   =====> Loss= 0.258191565\n",
      "Epoch:  21   =====> Loss= 0.248693671\n",
      "Epoch:  22   =====> Loss= 0.237663607\n",
      "Epoch:  23   =====> Loss= 0.233520937\n",
      "Epoch:  24   =====> Loss= 0.225409347\n",
      "Epoch:  25   =====> Loss= 0.218959628\n",
      "Epoch:  26   =====> Loss= 0.209345567\n",
      "Epoch:  27   =====> Loss= 0.207425835\n",
      "Epoch:  28   =====> Loss= 0.200050201\n",
      "Epoch:  29   =====> Loss= 0.194592636\n",
      "Epoch:  30   =====> Loss= 0.188732230\n",
      "Epoch:  31   =====> Loss= 0.185709067\n",
      "Epoch:  32   =====> Loss= 0.180496663\n",
      "Epoch:  33   =====> Loss= 0.178334349\n",
      "Epoch:  34   =====> Loss= 0.171407090\n",
      "Epoch:  35   =====> Loss= 0.166544871\n",
      "Epoch:  36   =====> Loss= 0.165257874\n",
      "Epoch:  37   =====> Loss= 0.162992557\n",
      "Epoch:  38   =====> Loss= 0.158820109\n",
      "Epoch:  39   =====> Loss= 0.153663999\n",
      "Epoch:  40   =====> Loss= 0.151041960\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "#STEP 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [batch_size,28, 28,1], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [batch_size, 10], name='LabelData')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = LeNet5_Model(data=x)\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            batch_xs = array(batch_xs).reshape(batch_size, 28,28,1)\n",
    "            #print(batch_xs.shape)\n",
    "            #print(batch_xs.dtype)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            \n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, y):\n",
    "    #your implementation goes here\n",
    "    correct_prediction = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    #print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 10 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "def train(learning_rate, training_epochs, batch_size, display_step, optimizer_method=tf.train.GradientDescentOptimizer,activation_function=tf.nn.relu):\n",
    "    tf.reset_default_graph()\n",
    "    # Initializing the session \n",
    "    \n",
    "    logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "    # tf Graph Input:  mnist data image of shape 28*28=784\n",
    "    x = tf.placeholder(tf.float32, [None,28,28,1], name='InputData')\n",
    "    # 0-9 digits recognition,  10 classes\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "    # Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "    with tf.name_scope('Model'):\n",
    "        # Model\n",
    "        pred = LeNet5_Model(data=x,activation_function=activation_function)\n",
    "    with tf.name_scope('Loss'):\n",
    "        # Minimize error using cross entropy\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred,-1.0,1.0)), reduction_indices=1))\n",
    "        #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    with tf.name_scope('SGD'):\n",
    "        # Gradient Descent\n",
    "        optimizer = optimizer_method(learning_rate).minimize(cost)\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        # Accuracy\n",
    "        acc = evaluate(pred, y)\n",
    "\n",
    "        \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"Loss\", cost)\n",
    "    # Create a summary to monitor accuracy tensor\n",
    "    tf.summary.scalar(\"Accuracy\", acc)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    startTime = time.time()\n",
    "    print (\"Start Training!\")\n",
    "    #t0 = time()\n",
    "    X_train,Y_train = mnist.train.images.reshape((-1,28,28,1)), mnist.train.labels\n",
    "    X_val,Y_val = mnist.validation.images.reshape((-1,28,28,1)), mnist.validation.labels\n",
    "    \n",
    "    # Launch the graph for training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                # train_next_batch shuffle the images by default\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                batch_xs = batch_xs.reshape((-1,28,28,1))\n",
    "\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                         feed_dict={x: batch_xs, \n",
    "                                                    y: batch_ys})\n",
    "                \n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "\n",
    "            # Display logs per epoch step\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"=====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "                                                \n",
    "                acc_train = acc.eval({x: X_train, y: Y_train})\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"=====> Accuracy Train=\", \"{:.9f}\".format(acc_train))\n",
    "                \n",
    "                acc_val = acc.eval({x: X_val, y: Y_val})\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"=====> Accuracy Validation=\", \"{:.9f}\".format(acc_val))\n",
    "        print (\"Training Finished!\")\n",
    "        #t1 = time()\n",
    "        # Save the variables to disk.\n",
    "        #save_path = saver.save(sess, \"model.ckpt\")\n",
    "        #print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        #Your implementation for testing accuracy after training goes here\n",
    "        \n",
    "        X_test,Y_test = mnist.test.images.reshape((-1,28,28,1)),mnist.test.labels\n",
    "\n",
    "        acc_test = acc.eval({x: X_test, y: Y_test})\n",
    "        print(\"Accuracy Test=\", \"{:.9f}\".format(acc_test))\n",
    "        endTime = time.time()\n",
    "        print(\"Time elapsed:\",endTime - startTime)\n",
    "        return acc_train,acc_val,acc_test,endTime - startTime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 6)\n",
      "(?, 14, 14, 6)\n",
      "(?, 10, 10, 16)\n",
      "(?, 5, 5, 16)\n",
      "(?, 400)\n",
      "Start Training!\n",
      "Epoch:  10 =====> Loss= 0.413092241\n",
      "Epoch:  10 =====> Accuracy Train= 0.889163613\n",
      "Epoch:  10 =====> Accuracy Validation= 0.893599987\n",
      "Epoch:  20 =====> Loss= 0.247280239\n",
      "Epoch:  20 =====> Accuracy Train= 0.928436339\n",
      "Epoch:  20 =====> Accuracy Validation= 0.934199989\n",
      "Epoch:  30 =====> Loss= 0.190800316\n",
      "Epoch:  30 =====> Accuracy Train= 0.942945480\n",
      "Epoch:  30 =====> Accuracy Validation= 0.950200021\n",
      "Epoch:  40 =====> Loss= 0.158061047\n",
      "Epoch:  40 =====> Accuracy Train= 0.953981817\n",
      "Epoch:  40 =====> Accuracy Validation= 0.958199978\n",
      "Training Finished!\n",
      "Accuracy Test= 0.959100008\n",
      "Time elapsed: 1202.9821767807007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9539818, 0.9582, 0.9591, 1202.9821767807007)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%time \n",
    "train(0.001,40,128,10,optimizer_method=tf.train.GradientDescentOptimizer,activation_function=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please put your loss and accuracy curves here.\n",
    "# Gradient Descent\n",
    "<img src=\"MNIST_figures/figure1_GD.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>\n",
    "\n",
    "- Retrain your network with AdamOptimizer and then fill the table above:\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |       0.9591       |       0.9898        |       \n",
    "| Training Time        |     1202.9821      |      408.4076       |  \n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?\n",
    "\n",
    "**Your answer:** \n",
    "Comparing the two, testing accuracy is higher with the Adam optimizer. However, note that we cut down number of epochs the Adam optimizer runs with because at 20 epochs, the results were already better than with Gradient Descent, and some time between 20 and 30 epochs, the optimizer seems to crash so that the accuracy drops significantly. We believe this is due to the value of epsilon, changing this should help. All that said, the training time between the two can't be compared apples to apples--although it seems much faster, it also had ran for half the epochs as gradient descent did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 6)\n",
      "(?, 14, 14, 6)\n",
      "(?, 10, 10, 16)\n",
      "(?, 5, 5, 16)\n",
      "(?, 400)\n",
      "Start Training!\n",
      "Epoch:  10 =====> Loss= 0.020745562\n",
      "Epoch:  10 =====> Accuracy Train= 0.993945479\n",
      "Epoch:  10 =====> Accuracy Validation= 0.988399982\n",
      "Epoch:  20 =====> Loss= 0.009232017\n",
      "Epoch:  20 =====> Accuracy Train= 0.996818185\n",
      "Epoch:  20 =====> Accuracy Validation= 0.988799989\n",
      "Training Finished!\n",
      "Accuracy Test= 0.989799976\n",
      "Time elapsed: 408.4076406955719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9968182, 0.9888, 0.9898, 408.4076406955719)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#%time\n",
    "train(0.001,20,128,10,optimizer_method=tf.train.AdamOptimizer,activation_function=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer\n",
    "<img src=\"MNIST_figures/figure2_Adam.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2</b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use tf.nn.dropout for that purpose. What accuracy do you achieve on testing data?\n",
    "\n",
    "**Accuracy achieved on testing data:** \n",
    "0.991999984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/\n",
    "# https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\n",
    "\n",
    "def LeNet5_Model_dropout(data,keep_prob,activation_function=tf.nn.relu):\n",
    "\n",
    "    # layer 1 param\n",
    "    conv1_weights = weight_variable([5,5,1,6])\n",
    "    conv1_bias = bias_variable([6])\n",
    "    \n",
    "    # layer 2 param\n",
    "    conv2_weights = weight_variable([5,5,6,16])\n",
    "    conv2_bias = bias_variable([16])\n",
    "    \n",
    "    # layer 3 param\n",
    "    layer3_weights = weight_variable([400, 120])\n",
    "    layer3_bias = bias_variable([120])\n",
    "    \n",
    "    # layer 4 param\n",
    "    layer4_weights = weight_variable([120, 84])\n",
    "    layer4_bias = bias_variable([84]) \n",
    "    \n",
    "    # layer 5 param\n",
    "    layer5_weights = weight_variable([84, 10])\n",
    "    layer5_bias = bias_variable([10])\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('Model'):\n",
    "        with tf.name_scope('Layer1'):\n",
    "            conv1 = tf.nn.conv2d(input=data,filter=conv1_weights,strides=[1,1,1,1],padding='SAME')\n",
    "            print(conv1.shape)\n",
    "            relu1 = activation_function(conv1 + conv1_bias)\n",
    "            pool1 = tf.nn.max_pool(relu1,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='VALID')\n",
    "            print(pool1.shape)\n",
    "            \n",
    "        with tf.name_scope('Layer2'):\n",
    "            conv2 = tf.nn.conv2d(input=pool1,filter=conv2_weights,strides=[1,1,1,1],padding='VALID')\n",
    "            print(conv2.shape)\n",
    "            relu2 = activation_function(conv2 + conv2_bias)\n",
    "            pool2 = tf.nn.max_pool(relu2,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='VALID')   \n",
    "            print(pool2.shape)\n",
    "        \n",
    "        with tf.name_scope('Flatten'):\n",
    "            flat_inputs = tf.contrib.layers.flatten(pool2)\n",
    "            print(flat_inputs.shape)\n",
    "           \n",
    "        with tf.name_scope('Layer3'): # first fully connected layer\n",
    "            out3 = activation_function(tf.matmul(flat_inputs, layer3_weights) + layer3_bias) # dropout called here\n",
    "            out3 = tf.nn.dropout(out3, keep_prob)\n",
    "            \n",
    "        with tf.name_scope('Layer4'):\n",
    "            out4 = activation_function(tf.matmul(out3, layer4_weights) + layer4_bias)\n",
    "            \n",
    "        with tf.name_scope('Layer5'):\n",
    "            pred = tf.nn.softmax(tf.matmul(out4, layer5_weights) + layer5_bias) # Softmax\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "def train_dropout(learning_rate, training_epochs, batch_size, display_step, optimizer_method=tf.train.GradientDescentOptimizer,activation_function=tf.nn.relu):\n",
    "    tf.reset_default_graph()\n",
    "    # Initializing the session \n",
    "    \n",
    "    logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "    # tf Graph Input:  mnist data image of shape 28*28=784\n",
    "    x = tf.placeholder(tf.float32, [None,28, 28,1], name='InputData')\n",
    "    # 0-9 digits recognition,  10 classes\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "    with tf.name_scope('Model'):\n",
    "        # Model\n",
    "        pred = LeNet5_Model_dropout(x,keep_prob,activation_function=activation_function)\n",
    "    with tf.name_scope('Loss'):\n",
    "        # Minimize error using cross entropy\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred,-1.0,1.0)), reduction_indices=1))\n",
    "        #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    with tf.name_scope('SGD'):\n",
    "        # Gradient Descent\n",
    "        optimizer = optimizer_method(learning_rate).minimize(cost)\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        # Accuracy\n",
    "        acc = evaluate(pred, y)\n",
    "\n",
    "        \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"Loss\", cost)\n",
    "    # Create a summary to monitor accuracy tensor\n",
    "    tf.summary.scalar(\"Accuracy\", acc)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    startTime = time.time()\n",
    "    print (\"Start Training!\")\n",
    "    #t0 = time()\n",
    "    X_train,Y_train = mnist.train.images.reshape((-1,28,28,1)), mnist.train.labels\n",
    "    X_val,Y_val = mnist.validation.images.reshape((-1,28,28,1)), mnist.validation.labels\n",
    "    \n",
    "    # Launch the graph for training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                # train_next_batch shuffle the images by default\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                batch_xs = batch_xs.reshape((-1,28,28,1))\n",
    "\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                         feed_dict={x: batch_xs, \n",
    "                                                    y: batch_ys,keep_prob:0.75})\n",
    "                \n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                \n",
    "\n",
    "            # Display logs per epoch step\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"=====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "                                                \n",
    "                acc_train = acc.eval({x: X_train, y: Y_train,keep_prob:1.0})\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"=====> Accuracy Train=\", \"{:.9f}\".format(acc_train))\n",
    "                \n",
    "                acc_val = acc.eval({x: X_val, y: Y_val,keep_prob:1.0})\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \"=====> Accuracy Validation=\", \"{:.9f}\".format(acc_val))\n",
    "        print (\"Training Finished!\")\n",
    "        #t1 = time()\n",
    "        # Save the variables to disk.\n",
    "        #save_path = saver.save(sess, \"model.ckpt\")\n",
    "        #print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        #Your implementation for testing accuracy after training goes here\n",
    "        \n",
    "        X_test,Y_test = mnist.test.images.reshape((-1,28,28,1)),mnist.test.labels\n",
    "\n",
    "        acc_test = acc.eval({x: X_test, y: Y_test,keep_prob:1.0})\n",
    "        print(\"Accuracy Test=\", \"{:.9f}\".format(acc_test))\n",
    "        endTime = time.time()\n",
    "        print(\"Time elapsed:\",endTime - startTime)\n",
    "        return acc_train,acc_val,acc_test,endTime - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 6)\n",
      "(?, 14, 14, 6)\n",
      "(?, 10, 10, 16)\n",
      "(?, 5, 5, 16)\n",
      "(?, 400)\n",
      "Start Training!\n",
      "Epoch:  10 =====> Loss= 0.026900856\n",
      "Epoch:  10 =====> Accuracy Train= 0.995163620\n",
      "Epoch:  10 =====> Accuracy Validation= 0.988399982\n",
      "Epoch:  20 =====> Loss= 0.012914957\n",
      "Epoch:  20 =====> Accuracy Train= 0.999036372\n",
      "Epoch:  20 =====> Accuracy Validation= 0.991999984\n",
      "Training Finished!\n",
      "Accuracy Test= 0.991999984\n",
      "Time elapsed: 637.7806880474091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9990364, 0.992, 0.992, 637.7806880474091)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dropout(0.001,20,128,10,optimizer_method=tf.train.AdamOptimizer,activation_function=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
